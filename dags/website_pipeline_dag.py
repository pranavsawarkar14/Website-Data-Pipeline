"""
Website Data Processing Pipeline DAG

This DAG orchestrates the complete website data processing pipeline:
1. Crawl websites and extract HTML content
2. Extract and clean text from HTML files
3. Transform data into standardized records
4. Aggregate metrics and compute insights

The pipeline is designed to be idempotent and supports retries.
"""

import os
import sys
from datetime import datetime, timedelta
from pathlib import Path

# Try to import Airflow modules, use mocks if not available
try:
    from airflow import DAG
    from airflow.operators.python import PythonOperator
    from airflow.operators.bash import BashOperator
    from airflow.utils.dates import days_ago
    from airflow.utils.task_group import TaskGroup
    from airflow.models import Variable
    AIRFLOW_AVAILABLE = True
except ImportError:
    # Create mock classes for testing without Airflow
    AIRFLOW_AVAILABLE = False
    
    class DAG:
        def __init__(self, *args, **kwargs):
            self.dag_id = kwargs.get('dag_id', 'mock_dag')
            self.description = kwargs.get('description', 'Mock DAG')
            self.schedule_interval = kwargs.get('schedule_interval', '@daily')
            self.task_dict = {}
            
        def __enter__(self):
            return self
            
        def __exit__(self, *args):
            pass
    
    class PythonOperator:
        def __init__(self, *args, **kwargs):
            self.task_id = kwargs.get('task_id', 'mock_task')
            
        def __rshift__(self, other):
            # Mock the >> operator for task dependencies
            return other
            
        def __lshift__(self, other):
            # Mock the << operator for task dependencies
            return self
            
    class BashOperator:
        def __init__(self, *args, **kwargs):
            self.task_id = kwargs.get('task_id', 'mock_task')
            
        def __rshift__(self, other):
            # Mock the >> operator for task dependencies
            return other
            
        def __lshift__(self, other):
            # Mock the << operator for task dependencies
            return self
    
    def days_ago(days):
        return datetime.now() - timedelta(days=days)
    
    class TaskGroup:
        def __init__(self, *args, **kwargs):
            pass
        def __enter__(self):
            return self
        def __exit__(self, *args):
            pass
    
    class Variable:
        @staticmethod
        def get(key, default_var=None):
            return default_var

# Add scripts directory to Python path
SCRIPTS_DIR = Path(__file__).resolve().parent.parent / "scripts"
sys.path.append(str(SCRIPTS_DIR))

# Default arguments for all tasks
default_args = {
    'owner': 'data-engineering-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2),
}

# DAG configuration
dag = DAG(
    'website_pipeline_dag',
    default_args=default_args,
    description='Complete website data processing pipeline',
    schedule_interval='@daily',  # Run daily
    start_date=days_ago(1),
    catchup=False,  # Don't run for past dates
    max_active_runs=1,  # Only one instance at a time
    tags=['website', 'data-processing', 'etl'],
)

def check_dependencies():
    """Check if required dependencies are available."""
    try:
        import requests
        import bs4
        print("âœ… All required dependencies are available")
        return True
    except ImportError as e:
        print(f"âŒ Missing dependency: {e}")
        raise

def crawl_websites_task():
    """Task to crawl websites and extract HTML content."""
    print("Starting website crawling...")
    
    try:
        # Import and run crawler
        import crawler
        crawler.main()
        print("âœ… Website crawling completed successfully")
        
        # Verify output exists
        from pathlib import Path
        raw_dir = Path(__file__).resolve().parent.parent / "data" / "raw"
        if raw_dir.exists() and any(raw_dir.iterdir()):
            print(f"âœ… Raw data directory contains {len(list(raw_dir.iterdir()))} domains")
        else:
            raise Exception("No raw data was generated by crawler")
            
    except Exception as e:
        print(f"âŒ Crawling failed: {e}")
        raise

def extract_and_tag_task():
    """Task to extract clean text from HTML files."""
    print("Starting text extraction...")
    
    try:
        # Import and run extractor
        import extractor
        extractor.main()
        print("âœ… Text extraction completed successfully")
        
        # Verify output exists
        from pathlib import Path
        import json
        
        extracted_file = Path(__file__).resolve().parent.parent / "data" / "processed" / "extracted.json"
        if extracted_file.exists():
            with open(extracted_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            domains_count = len(data.get('domains', {}))
            print(f"âœ… Extracted text from {domains_count} domains")
        else:
            raise Exception("No extracted data file was generated")
            
    except Exception as e:
        print(f"âŒ Text extraction failed: {e}")
        raise

def standardize_data_task():
    """Task to transform data into standardized records."""
    print("Starting data standardization...")
    
    try:
        # Import and run transformer
        import transformer
        transformer.main()
        print("âœ… Data standardization completed successfully")
        
        # Verify output exists
        from pathlib import Path
        import json
        
        standardized_file = Path(__file__).resolve().parent.parent / "data" / "processed" / "standardized.json"
        if standardized_file.exists():
            with open(standardized_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            records_count = len(data.get('records', []))
            print(f"âœ… Created {records_count} standardized records")
        else:
            raise Exception("No standardized data file was generated")
            
    except Exception as e:
        print(f"âŒ Data standardization failed: {e}")
        raise

def aggregate_metrics_task():
    """Task to compute aggregated metrics and insights."""
    print("Starting metrics aggregation...")
    
    try:
        # Import and run aggregator
        import aggregator
        aggregator.main()
        print("âœ… Metrics aggregation completed successfully")
        
        # Verify output exists and show key metrics
        from pathlib import Path
        import json
        
        metrics_file = Path(__file__).resolve().parent.parent / "data" / "aggregated" / "metrics.json"
        if metrics_file.exists():
            with open(metrics_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Log key findings
            case_study = data.get('case_study_analysis', {})
            activity = data.get('activity_analysis', {})
            additional = data.get('additional_metrics', {})
            
            print("ðŸ“Š Pipeline Results Summary:")
            print(f"  Total websites: {additional.get('unique_websites', 0)}")
            print(f"  Total records: {additional.get('total_records', 0)}")
            print(f"  Websites with case studies: {case_study.get('websites_with_case_studies', 0)}/{case_study.get('total_websites', 0)}")
            print(f"  Active websites: {activity.get('active_websites', 0)}/{activity.get('total_websites', 0)}")
            print(f"  Content fill rate: {additional.get('content_fill_rate', 0)}%")
            print(f"  Validation passed: {data.get('validation_passed', False)}")
        else:
            raise Exception("No metrics file was generated")
            
    except Exception as e:
        print(f"âŒ Metrics aggregation failed: {e}")
        raise

def cleanup_logs_task():
    """Optional task to clean up old log files."""
    print("Cleaning up old log files...")
    
    try:
        from pathlib import Path
        import time
        
        base_dir = Path(__file__).resolve().parent.parent
        log_files = [
            base_dir / "crawler.log",
            base_dir / "extractor.log", 
            base_dir / "transformer.log",
            base_dir / "aggregator.log"
        ]
        
        # Keep logs from last 7 days, archive older ones
        cutoff_time = time.time() - (7 * 24 * 60 * 60)  # 7 days ago
        
        for log_file in log_files:
            if log_file.exists():
                file_time = log_file.stat().st_mtime
                if file_time < cutoff_time:
                    # Archive old log
                    archive_name = f"{log_file.stem}_{datetime.fromtimestamp(file_time).strftime('%Y%m%d')}.log"
                    archive_path = log_file.parent / "logs" / archive_name
                    archive_path.parent.mkdir(exist_ok=True)
                    log_file.rename(archive_path)
                    print(f"Archived old log: {archive_name}")
        
        print("âœ… Log cleanup completed")
        
    except Exception as e:
        print(f"âš ï¸ Log cleanup failed (non-critical): {e}")
        # Don't fail the DAG for log cleanup issues

def validate_pipeline_output():
    """Validate that all pipeline outputs are present and valid."""
    print("Validating pipeline outputs...")
    
    try:
        from pathlib import Path
        import json
        
        base_dir = Path(__file__).resolve().parent.parent
        
        # Check all expected output files
        expected_files = [
            base_dir / "data" / "metadata.json",
            base_dir / "data" / "processed" / "extracted.json",
            base_dir / "data" / "processed" / "standardized.json",
            base_dir / "data" / "aggregated" / "metrics.json"
        ]
        
        for file_path in expected_files:
            if not file_path.exists():
                raise Exception(f"Missing expected output file: {file_path}")
            
            # Validate JSON files
            if file_path.suffix == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    json.load(f)  # Will raise exception if invalid JSON
        
        # Check raw data directory
        raw_dir = base_dir / "data" / "raw"
        if not raw_dir.exists() or not any(raw_dir.iterdir()):
            raise Exception("Raw data directory is empty")
        
        print("âœ… All pipeline outputs are valid")
        
    except Exception as e:
        print(f"âŒ Pipeline validation failed: {e}")
        raise

# Task definitions
with dag:
    
    # Dependency check task
    check_deps = PythonOperator(
        task_id='check_dependencies',
        python_callable=check_dependencies,
        doc_md="""
        ## Check Dependencies
        Verifies that all required Python packages are installed.
        """,
    )
    
    # Main pipeline tasks
    crawl_websites = PythonOperator(
        task_id='crawl_websites',
        python_callable=crawl_websites_task,
        doc_md="""
        ## Crawl Websites
        Crawls configured websites and extracts HTML content.
        - Fetches homepage, navbar, footer, and case study pages
        - Saves raw HTML to data/raw/<domain>/
        - Creates metadata.json with crawl information
        """,
    )
    
    extract_and_tag = PythonOperator(
        task_id='extract_and_tag',
        python_callable=extract_and_tag_task,
        doc_md="""
        ## Extract and Tag
        Extracts clean text from HTML files using intelligent heuristics.
        - Removes scripts, styles, and comments
        - Extracts navbar, homepage, footer, and case study content
        - Saves to data/processed/extracted.json
        """,
    )
    
    standardize_data = PythonOperator(
        task_id='standardize_data',
        python_callable=standardize_data_task,
        doc_md="""
        ## Standardize Data
        Transforms extracted text into standardized records.
        - Creates individual records for each website section
        - Maps domains to original URLs
        - Preserves crawl timestamps
        - Saves to data/processed/standardized.json
        """,
    )
    
    aggregate_metrics = PythonOperator(
        task_id='aggregate_metrics',
        python_callable=aggregate_metrics_task,
        doc_md="""
        ## Aggregate Metrics
        Computes business metrics and insights from standardized data.
        - Case study analysis
        - Activity analysis
        - Content length metrics by section
        - Saves to data/aggregated/metrics.json
        """,
    )
    
    # Validation and cleanup tasks
    validate_output = PythonOperator(
        task_id='validate_pipeline_output',
        python_callable=validate_pipeline_output,
        doc_md="""
        ## Validate Output
        Validates that all expected pipeline outputs are present and valid.
        """,
    )
    
    cleanup_logs = PythonOperator(
        task_id='cleanup_logs',
        python_callable=cleanup_logs_task,
        trigger_rule='none_failed',  # Run even if some tasks failed
        doc_md="""
        ## Cleanup Logs
        Archives old log files to keep the workspace clean.
        """,
    )
    
    # Alternative bash operators for running scripts directly (commented out)
    # These can be used instead of Python operators if preferred
    """
    crawl_websites_bash = BashOperator(
        task_id='crawl_websites_bash',
        bash_command='cd {{ dag.folder }}/../ && python scripts/crawler.py',
        retries=2,
    )
    
    extract_and_tag_bash = BashOperator(
        task_id='extract_and_tag_bash', 
        bash_command='cd {{ dag.folder }}/../ && python scripts/extractor.py',
        retries=2,
    )
    
    standardize_data_bash = BashOperator(
        task_id='standardize_data_bash',
        bash_command='cd {{ dag.folder }}/../ && python scripts/transformer.py',
        retries=2,
    )
    
    aggregate_metrics_bash = BashOperator(
        task_id='aggregate_metrics_bash',
        bash_command='cd {{ dag.folder }}/../ && python scripts/aggregator.py',
        retries=2,
    )
    """

# Task dependencies - Linear pipeline with validation
check_deps >> crawl_websites >> extract_and_tag >> standardize_data >> aggregate_metrics >> validate_output >> cleanup_logs

# Alternative: Parallel validation (uncomment if desired)
# check_deps >> crawl_websites >> extract_and_tag >> standardize_data >> aggregate_metrics
# [aggregate_metrics, validate_output] >> cleanup_logs